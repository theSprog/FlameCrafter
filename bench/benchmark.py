#!/usr/bin/env python3
"""
ç«ç„°å›¾ç”Ÿæˆå™¨æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
ç”¨äºå¯¹æ¯”ä¸åŒè§„æ¨¡æ•°æ®çš„å¤„ç†æ€§èƒ½
"""

import subprocess
import time
import os
import sys
from typing import Dict, List, Tuple

class FlameGraphBenchmark:
    def __init__(self):
        BASE_DIR = os.path.dirname(os.path.abspath(__file__))
        self.test_configs = [
            ("small", 1000, "1K"),
            ("medium", 10000, "10K"), 
            ("large", 100000, "100K"),
        ]
        print("BASE_DIR: ", BASE_DIR)
        self.executable = os.path.join(BASE_DIR, "../flamegraph_main")
        self.generate_data_script = os.path.join(BASE_DIR, "../script/generate_test_data.py")
        
    def ensure_executable_exists(self) -> bool:
        """ç¡®ä¿å¯æ‰§è¡Œæ–‡ä»¶å­˜åœ¨"""
        if not os.path.exists(self.executable):
            print("âŒ Executable not found. Building...")
            result = subprocess.run(["make", "all"], capture_output=True, text=True)
            if result.returncode != 0:
                print(f"âŒ Build failed: {result.stderr}")
                return False
            print("âœ… Build successful")
        return True
    
    def generate_test_data(self, size_name: str, sample_count: int) -> str:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        test_file = f"{size_name}_test.txt"
        
        print(f"ğŸ“Š Generating {size_name} test data ({sample_count:,} samples)...")
        
        cmd = [
            "python3", self.generate_data_script, 
            "--output", test_file,
            "--samples", str(sample_count)
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            raise RuntimeError(f"Failed to generate test data: {result.stderr}")
        
        return test_file
    
    def run_benchmark(self, input_file: str, output_file: str) -> Dict[str, float]:
        """è¿è¡Œå•ä¸ªåŸºå‡†æµ‹è¯•"""
        # é¢„çƒ­è¿è¡Œ
        subprocess.run([self.executable, input_file, output_file], 
                      capture_output=True, text=True)
        
        # æ­£å¼æµ‹è¯• - è¿è¡Œ3æ¬¡å–å¹³å‡å€¼
        times = []
        for i in range(3):
            start_time = time.time()
            result = subprocess.run(
                [self.executable, input_file, output_file],
                capture_output=True, text=True
            )
            end_time = time.time()
            
            if result.returncode != 0:
                raise RuntimeError(f"Benchmark failed: {result.stderr}")
            
            times.append(end_time - start_time)
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        
        return {
            "avg": avg_time,
            "min": min_time, 
            "max": max_time,
            "times": times
        }
    
    def get_file_info(self, filename: str) -> Dict[str, any]:
        """è·å–æ–‡ä»¶ä¿¡æ¯"""
        if not os.path.exists(filename):
            return {"size": 0, "lines": 0}
        
        file_size = os.path.getsize(filename)
        
        # è®¡ç®—è¡Œæ•°
        line_count = 0
        with open(filename, 'r') as f:
            for _ in f:
                line_count += 1
        
        return {
            "size": file_size,
            "lines": line_count
        }
    
    def format_time(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
        if seconds < 1:
            return f"{seconds*1000:.1f}ms"
        else:
            return f"{seconds:.2f}s"
    
    def format_size(self, bytes_size: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if bytes_size < 1024:
            return f"{bytes_size}B"
        elif bytes_size < 1024 * 1024:
            return f"{bytes_size/1024:.1f}KB"
        else:
            return f"{bytes_size/1024/1024:.1f}MB"
    
    def run_all_benchmarks(self) -> List[Tuple[str, Dict]]:
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        results = []
        
        print("ğŸ”¥ Starting Flame Graph Performance Benchmark")
        print("=" * 50)
        
        for size_name, sample_count, display_name in self.test_configs:
            print(f"\nğŸ“Š Testing {display_name} dataset...")
            print("-" * 30)
            
            try:
                # ç”Ÿæˆæµ‹è¯•æ•°æ®
                input_file = self.generate_test_data(size_name, sample_count)
                output_file = f"{size_name}_benchmark.svg"
                
                # è·å–è¾“å…¥æ–‡ä»¶ä¿¡æ¯
                file_info = self.get_file_info(input_file)
                print(f"   Input file: {self.format_size(file_info['size'])}, {file_info['lines']:,} lines")
                
                # è¿è¡ŒåŸºå‡†æµ‹è¯•
                print("   Running benchmark...")
                benchmark_result = self.run_benchmark(input_file, output_file)
                
                # è·å–è¾“å‡ºæ–‡ä»¶ä¿¡æ¯
                output_info = self.get_file_info(output_file)
                
                # è®°å½•ç»“æœ
                result = {
                    "size_name": size_name,
                    "display_name": display_name,
                    "sample_count": sample_count,
                    "input_size": file_info["size"],
                    "input_lines": file_info["lines"],
                    "output_size": output_info["size"],
                    "benchmark": benchmark_result
                }
                results.append((display_name, result))
                
                # æ˜¾ç¤ºç»“æœ
                print(f"   âœ… Average time: {self.format_time(benchmark_result['avg'])}")
                print(f"   ğŸ“ˆ Range: {self.format_time(benchmark_result['min'])} - {self.format_time(benchmark_result['max'])}")
                print(f"   ğŸ“„ Output SVG: {self.format_size(output_info['size'])}")
                
                # è®¡ç®—å¤„ç†é€Ÿåº¦
                samples_per_sec = sample_count / benchmark_result['avg']
                print(f"   âš¡ Processing rate: {samples_per_sec:,.0f} samples/sec")
                
            except Exception as e:
                print(f"   âŒ Failed: {e}")
                continue
        
        return results
    
    def print_summary(self, results: List[Tuple[str, Dict]]):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        if not results:
            print("\nâŒ No benchmark results to summarize")
            return
        
        print("\n" + "=" * 60)
        print("ğŸ† BENCHMARK SUMMARY")
        print("=" * 60)
        
        print(f"{'Dataset':<10} {'Samples':<10} {'Input':<10} {'Time':<12} {'Rate':<15} {'Output':<10}")
        print("-" * 70)
        
        for display_name, result in results:
            avg_time = result["benchmark"]["avg"]
            rate = result["sample_count"] / avg_time
            
            print(f"{display_name:<10} "
                  f"{result['sample_count']:>9,} "
                  f"{self.format_size(result['input_size']):<10} "
                  f"{self.format_time(avg_time):<12} "
                  f"{rate:>10,.0f} smp/s "
                  f"{self.format_size(result['output_size']):<10}")
        
        # æ€§èƒ½åˆ†æ
        if len(results) >= 2:
            print(f"\nğŸ“ˆ Performance Analysis:")
            small_rate = results[0][1]["sample_count"] / results[0][1]["benchmark"]["avg"]
            for i in range(1, len(results)):
                current_rate = results[i][1]["sample_count"] / results[i][1]["benchmark"]["avg"]
                ratio = current_rate / small_rate
                print(f"   {results[i][0]} is {ratio:.1f}x as efficient as {results[0][0]} (per sample)")

def main():
    benchmark = FlameGraphBenchmark()
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not benchmark.ensure_executable_exists():
        return 1
    
    try:
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        results = benchmark.run_all_benchmarks()
        
        # æ‰“å°æ€»ç»“
        benchmark.print_summary(results)
        
        print(f"\nğŸ‰ Benchmark completed! Generated {len(results)} test results.")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Benchmark interrupted by user")
        return 1
    except Exception as e:
        print(f"\nâŒ Benchmark failed: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())